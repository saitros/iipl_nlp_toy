{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example2 | PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. GPU Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions download -c quora-insincere-questions-classification\n",
    "# !unzip ./data/quora-insincere-questions-classification.zip\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "sample_submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                         qid  \\\n",
       "0      00002165364db923c7e6   \n",
       "1      000032939017120e6e44   \n",
       "2      0000412ca6e4628ce2cf   \n",
       "3      000042bf85aa498cd78e   \n",
       "4      0000455dfa3e01eae3af   \n",
       "5      00004f9a462a357c33be   \n",
       "6      00005059a06ee19e11ad   \n",
       "7      0000559f875832745e2e   \n",
       "8      00005bd3426b2d0c8305   \n",
       "9      00006e6928c5df60eacb   \n",
       "10     000075f67dd595c3deb5   \n",
       "11     000076f3b42776c692de   \n",
       "12     000089792b3fc8026741   \n",
       "13     000092a90bcfbfe8cd88   \n",
       "14     000095680e41a9a6f6e3   \n",
       "15     0000a89942e3143e333a   \n",
       "16     0000b8e1279eaa0a7062   \n",
       "17     0000bc0f62500f55959f   \n",
       "18     0000ce6c31f14d3e09ec   \n",
       "19     0000d329332845b8a7fa   \n",
       "20     0000dd973dfd35508c16   \n",
       "21     0000e4d455f9c8877dc9   \n",
       "22     0000e91571b60c2fb487   \n",
       "23     000101ac65db6e4a1c13   \n",
       "24     00010632971fe5e3e0e2   \n",
       "25     00010a2e064c3e8f152a   \n",
       "26     00012011b6c7759461e8   \n",
       "27     00012fd5128d576260ab   \n",
       "28     0001303b1799a042b26b   \n",
       "29     00013a8152b5f37b780e   \n",
       "...                     ...   \n",
       "29970  05dc1ccd3312b71a48f6   \n",
       "29971  05dc3410f08190cf883f   \n",
       "29972  05dc410dd1d39963f2b6   \n",
       "29973  05dc5a2f5901a5d0f61e   \n",
       "29974  05dc665f58cd7ac3b6a3   \n",
       "29975  05dc6e0e29bb943f9d74   \n",
       "29976  05dc81d8b2baf5e98bc0   \n",
       "29977  05dc97b59fc47abd7aaf   \n",
       "29978  05dc99cbdcbab4115f5c   \n",
       "29979  05dc9f88a31fce2af990   \n",
       "29980  05dca441fdfb8807ccf7   \n",
       "29981  05dcae80e5b5fdb745ee   \n",
       "29982  05dcdedf37520da9954a   \n",
       "29983  05dce1e7ad7fae785be6   \n",
       "29984  05dce3ae608815e8c75d   \n",
       "29985  05dd008f27e2848d35cc   \n",
       "29986  05dd095fbeb6f22fc358   \n",
       "29987  05dd0bcb8532e5845493   \n",
       "29988  05dd0d6ae26cbad28347   \n",
       "29989  05dd13ca23e431413b8f   \n",
       "29990  05dd182d4b9d989ffe78   \n",
       "29991  05dd18e2017631dce065   \n",
       "29992  05dd1fd20419b543981a   \n",
       "29993  05dd2b199242b3e0fb1f   \n",
       "29994  05dd3e8b40016c79e74b   \n",
       "29995  05dd4bb37eaec1006906   \n",
       "29996  05dd4ec56c049e86d0a6   \n",
       "29997  05dd5064bab6f9e72dfc   \n",
       "29998  05dd537f177f4a00c05d   \n",
       "29999  05dd53aa289427c8c7de   \n",
       "\n",
       "                                           question_text  target  \n",
       "0      How did Quebec nationalists see their province...       0  \n",
       "1      Do you have an adopted dog, how would you enco...       0  \n",
       "2      Why does velocity affect time? Does velocity a...       0  \n",
       "3      How did Otto von Guericke used the Magdeburg h...       0  \n",
       "4      Can I convert montra helicon D to a mountain b...       0  \n",
       "5      Is Gaza slowly becoming Auschwitz, Dachau or T...       0  \n",
       "6      Why does Quora automatically ban conservative ...       0  \n",
       "7      Is it crazy if I wash or wipe my groceries off...       0  \n",
       "8      Is there such a thing as dressing moderately, ...       0  \n",
       "9      Is it just me or have you ever been in this ph...       0  \n",
       "10                      What can you say about feminism?       0  \n",
       "11                  How were the Calgary Flames founded?       0  \n",
       "12     What is the dumbest, yet possibly true explana...       0  \n",
       "13     Can we use our external hard disk as a OS as w...       0  \n",
       "14     I am 30, living at home and have no boyfriend....       0  \n",
       "15     What do you know about Bram Fischer and the Ri...       0  \n",
       "16     How difficult is it to find a good instructor ...       0  \n",
       "17                 Have you licked the skin of a corpse?       0  \n",
       "18     Do you think Amazon will adopt an in house app...       0  \n",
       "19     How many baronies might exist within a county ...       0  \n",
       "20     How I know whether a girl had done sex before ...       0  \n",
       "21     How do I become a fast learner both in my prof...       0  \n",
       "22     Has the United States become the largest dicta...       1  \n",
       "23     What is the strangest phenomenon you know of, ...       0  \n",
       "24          Should I leave my friends and find new ones?       0  \n",
       "25     Can you make Amazon Alexa trigger events in th...       0  \n",
       "26     Why haven't two democracies never ever went fo...       0  \n",
       "27                       How can I top CBSE in 6 months?       0  \n",
       "28     What should I know before visiting Mcleodganj ...       0  \n",
       "29     How do modern military submarines reduce noise...       0  \n",
       "...                                                  ...     ...  \n",
       "29970             Is there any way to obtain stem cells?       0  \n",
       "29971      What are some ''feel good'' Bollywood movies?       0  \n",
       "29972  What would happen if one of America's carrier ...       0  \n",
       "29973  Parents: do you regret it when you yell at you...       0  \n",
       "29974                  How does the music business work?       0  \n",
       "29975  What is the difference between Newton's and hu...       0  \n",
       "29976  How can I whiten my teeth without going to the...       0  \n",
       "29977  What techniques did Jeff Harding use while act...       0  \n",
       "29978  Why doesn't the CS courses at community colleg...       0  \n",
       "29979  Imagine if Jiraiya was injured and taken back ...       0  \n",
       "29980  How quickly do rapidly growing plants, like Co...       0  \n",
       "29981  What kind of research happens at Dakshin Gango...       0  \n",
       "29982  Does playing heavy games for long time cause t...       0  \n",
       "29983                       What is a limitation of SMS?       0  \n",
       "29984  What caused a young, healthy celebrity to turn...       0  \n",
       "29985  Is Fortnite the most successful game of the la...       0  \n",
       "29986  What are the best Steakhouse restaurants to ea...       0  \n",
       "29987  Which countries are the coming top investors i...       0  \n",
       "29988  What are some free tools to aid my brochure de...       0  \n",
       "29989             What do you use your pickup truck for?       0  \n",
       "29990   What is the scope of forensic sciences in India?       0  \n",
       "29991  My casual relationship of 2 years has ended no...       0  \n",
       "29992  What books to follow for nabard assistant mana...       0  \n",
       "29993  What are the three best way to grow my travel ...       0  \n",
       "29994  My friend and I made a silly music video. I bo...       0  \n",
       "29995               How do I access my Hotmail accouhnt?       0  \n",
       "29996         Do the rich still need to wipe their butt?       0  \n",
       "29997  How many children are registered for homeschoo...       0  \n",
       "29998  How much money will FIITJEE Ghaziabad give to ...       0  \n",
       "29999  How do I design a bell crank for a formula stu...       0  \n",
       "\n",
       "[30000 rows x 3 columns]>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.iloc[:30000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def alpha_num(text):\n",
    "    return re.sub(r'[^a-zA-z0-9\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = train['question_text'].str.lower().apply(alpha_num)\n",
    "test_text_list = test['question_text'].str.lower().apply(alpha_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train & validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "valid_percent = 0.2\n",
    "\n",
    "data_len = len(train)\n",
    "test_index = list(range(len(test)))\n",
    "\n",
    "valid_index = np.random.choice(range(data_len), int(data_len*valid_percent), replace=False)\n",
    "train_index = list(set(range(data_len)) - set(valid_index))\n",
    "\n",
    "train_text_list = [text_list[i] for i in train_index]\n",
    "valid_text_list = [text_list[i] for i in valid_index]\n",
    "test_text_list = [test_text_list[i] for i in test_index]\n",
    "\n",
    "train_label_list = [train['target'].tolist()[i] for i in train_index]\n",
    "valid_label_list = [train['target'].tolist()[i] for i in valid_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_path = './save'\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "vocab_size = 12000\n",
    "pad_idx = 0\n",
    "bos_idx = 1\n",
    "eos_idx = 2\n",
    "unk_idx = 3\n",
    "\n",
    "if not os.path.isfile(f'{save_path}/m_text.vocab'):\n",
    "    # 1) Make Korean text to train vocab\n",
    "    with open(f'{save_path}/text.txt', 'w') as f:\n",
    "        for text in train_text_list:\n",
    "            f.write(f'{text}\\n')\n",
    "\n",
    "\n",
    "    # 2) SentencePiece model training\n",
    "    spm.SentencePieceProcessor()\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f'--input={save_path}/text.txt --model_prefix={save_path}/m_text'\n",
    "        f'--vocab_size={vocab_size} --character_coverage=0.9995 '\n",
    "        f'--model_type=bpe --split_by_whitespace=true '\n",
    "        f'--pad_id={pad_idx} --unk_id={unk_idx} '\n",
    "        f'--bos_id={bos_idx} --eos_id={eos_idx}'\n",
    "    )\n",
    "\n",
    "    vocab_list = list()\n",
    "    with open(f'{save_path}/m_text_{vocab_size}.vocab') as f:\n",
    "        for line in f:\n",
    "            vocab_list.append(line[:-1].split('\\t')[0])\n",
    "    word2id_spm = {w: i for i, w in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentencePiece model load\n",
    "spm_ = spm.SentencePieceProcessor()\n",
    "spm_.Load(f\"{save_path}/m_text.model\")\n",
    "\n",
    "# Tokenizing\n",
    "train_encoded_list = [[bos_idx] + spm_.EncodeAsIds(text) + [eos_idx] for text in train_text_list]\n",
    "valid_encoded_list = [[bos_idx] + spm_.EncodeAsIds(text) + [eos_idx] for text in valid_text_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, src_list, trg_list, min_len=4, max_len=500):\n",
    "        data = list()\n",
    "        for src, trg in zip(src_list, trg_list):\n",
    "            if min_len <= len(src) <= max_len:\n",
    "                data.append((src, trg))\n",
    "\n",
    "        self.data = data\n",
    "        self.num_data = len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        src, trg = self.data[index]\n",
    "        return src, trg\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data\n",
    "\n",
    "class PadCollate:\n",
    "    def __init__(self, pad_index=0, dim=0):\n",
    "        self.dim = dim\n",
    "        self.pad_index = pad_index\n",
    "\n",
    "    def pad_collate(self, batch):\n",
    "        def pad_tensor(vec, max_len, dim):\n",
    "            pad_size = list(vec.shape)\n",
    "            pad_size[dim] = max_len - vec.size(dim)\n",
    "            return torch.cat([vec, torch.LongTensor(*pad_size).fill_(self.pad_index)], dim=dim)\n",
    "\n",
    "        def pack_sentence(sentences):\n",
    "            sentences_len = max(map(lambda x: len(x), sentences))\n",
    "            sentences = [pad_tensor(torch.LongTensor(seq), sentences_len, self.dim) for seq in sentences]\n",
    "            sentences = torch.cat(sentences)\n",
    "            sentences = sentences.view(-1, sentences_len)\n",
    "            return sentences\n",
    "\n",
    "        src, trg = zip(*batch)\n",
    "        return pack_sentence(src), torch.LongTensor(trg)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self.pad_collate(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainingsets  iterations - 24000, 3000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "dataset_dict = {\n",
    "    'train': CustomDataset(train_encoded_list, train_label_list, min_len=4, max_len=500),\n",
    "    'valid': CustomDataset(valid_encoded_list, valid_label_list, min_len=4, max_len=500),\n",
    "}\n",
    "\n",
    "dataloader_dict = {\n",
    "    'train': DataLoader(dataset_dict['train'], collate_fn=PadCollate(), drop_last=True,\n",
    "                        batch_size=batch_size, num_workers=4, shuffle=True, pin_memory=True),\n",
    "    'valid': DataLoader(dataset_dict['valid'], collate_fn=PadCollate(), drop_last=True,\n",
    "                        batch_size=batch_size, num_workers=4, shuffle=True, pin_memory=True),\n",
    "}\n",
    "\n",
    "print(f'Total number of trainingsets  iterations - {len(dataset_dict[\"train\"])}, {len(dataloader_dict[\"train\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, src_vocab_num, trg_num=2, pad_idx=0, bos_idx=1, eos_idx=2, \n",
    "                 max_len=500, d_model=512, d_embedding=256, dropout=0.1):\n",
    "\n",
    "        super(CustomModel, self).__init__()\n",
    "\n",
    "        self.src_embedding = nn.Embedding(src_vocab_num, d_embedding, padding_idx=pad_idx)\n",
    "        self.linear1 = nn.Linear(d_embedding, d_model)\n",
    "        self.linear2 = nn.Linear(d_model, trg_num)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "\n",
    "        embedding_out = self.src_embedding(src)\n",
    "        gap_out = embedding_out.mean(dim=1)\n",
    "        linear_out = self.dropout(F.gelu(self.linear1(gap_out)))\n",
    "        logit = self.linear2(linear_out)\n",
    "\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (src_embedding): Embedding(12000, 256, padding_idx=0)\n",
       "  (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (linear2): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CustomModel(vocab_size, trg_num=2, pad_idx=pad_idx, bos_idx=bos_idx, eos_idx=eos_idx,\n",
    "                    max_len=500, d_model=512, d_embedding=256, dropout=0.1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Latent Variable --> attn?\n",
    "## \n",
    "\n",
    "class CustomModel_Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p, output_size, pad_idx=0):\n",
    "        super(CustomModel_Encoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "        self.attn = nn.Linear(hidden_size, ???)\n",
    "        \n",
    "        self.out = nn.Linear(, self.output_size)\n",
    "\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        \n",
    "        attn_weight = self.attn(hidden, ???)\n",
    "        \n",
    "        output = F.log_softmax()\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def initHidden(self):\n",
    "        \"\"\"\n",
    "        torch.zeros(shape_1, shape_2, shape_3, device)\n",
    "        \"\"\"\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel_Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, tar_num=2):\n",
    "        super(CustomModel_Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.attn = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        self.attn = nn.Linear(self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimizer setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "lr = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, \n",
    "                              patience=len(dataloader_dict['train'])/1.5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Fitting: [1/5]\n",
      "[Epoch:1][0/3000] train_loss:0.693 | Accuracy:0.625 | lr:0.010000 | spend_time: 0.03min\n",
      "[Epoch:1][1500/3000] train_loss:0.340 | Accuracy:0.875 | lr:0.010000 | spend_time: 0.13min\n",
      "[Epoch:1] val_loss:0.185 | Accuracy: 0.94 | spend_time: 0.28min\n",
      "[!] saving model...\n",
      "Model Fitting: [2/5]\n",
      "[Epoch:2][0/3000] train_loss:0.056 | Accuracy:1.000 | lr:0.010000 | spend_time: 0.01min\n",
      "[Epoch:2][1500/3000] train_loss:0.016 | Accuracy:1.000 | lr:0.010000 | spend_time: 0.12min\n",
      "[Epoch:2] val_loss:0.184 | Accuracy: 0.94 | spend_time: 0.27min\n",
      "[!] saving model...\n",
      "Model Fitting: [3/5]\n",
      "[Epoch:3][0/3000] train_loss:0.160 | Accuracy:1.000 | lr:0.010000 | spend_time: 0.01min\n",
      "[Epoch:3][1500/3000] train_loss:0.644 | Accuracy:0.750 | lr:0.001000 | spend_time: 0.11min\n",
      "[Epoch:3] val_loss:0.181 | Accuracy: 0.94 | spend_time: 0.27min\n",
      "[!] saving model...\n",
      "Model Fitting: [4/5]\n",
      "[Epoch:4][0/3000] train_loss:0.109 | Accuracy:1.000 | lr:0.000100 | spend_time: 0.01min\n",
      "[Epoch:4][1500/3000] train_loss:0.105 | Accuracy:1.000 | lr:0.000010 | spend_time: 0.11min\n",
      "[Epoch:4] val_loss:0.182 | Accuracy: 0.94 | spend_time: 0.27min\n",
      "Model Fitting: [5/5]\n",
      "[Epoch:5][0/3000] train_loss:0.026 | Accuracy:1.000 | lr:0.000010 | spend_time: 0.01min\n",
      "[Epoch:5][1500/3000] train_loss:0.018 | Accuracy:1.000 | lr:0.000001 | spend_time: 0.11min\n",
      "[Epoch:5] val_loss:0.181 | Accuracy: 0.94 | spend_time: 0.27min\n",
      "[!] saving model...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "num_epoch = 5\n",
    "\n",
    "print_freq = 1500\n",
    "best_val_loss = None\n",
    "\n",
    "for e in range(num_epoch):\n",
    "    start_time_e = time.time()\n",
    "    print(f'Model Fitting: [{e+1}/{num_epoch}]')\n",
    "    for phase in ['train', 'valid']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "            freq = 0\n",
    "        elif phase == 'valid':\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_acc = 0\n",
    "\n",
    "        for i, (src, trg) in enumerate(dataloader_dict[phase]):\n",
    "            # Optimizer setting\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Source, Target sentence setting\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            # Model / Calculate loss\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                predicted_logit = model(src)\n",
    "                # If phase train, then backward loss and step optimizer and scheduler\n",
    "                if phase == 'train':\n",
    "                    loss = criterion(predicted_logit, trg)\n",
    "                    loss.backward()\n",
    "                    clip_grad_norm_(model.parameters(), 5)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step(loss)\n",
    "                    # Print loss value only training\n",
    "                    if freq == print_freq or freq == 0 or i == len(dataloader_dict['train']):\n",
    "                        total_loss = loss.item()\n",
    "                        _, predicted = predicted_logit.max(dim=1)\n",
    "                        accuracy = sum(predicted == trg).item() / predicted.size(0)\n",
    "                        print(\"[Epoch:%d][%d/%d] train_loss:%5.3f | Accuracy:%2.3f | lr:%1.6f | spend_time:%5.2fmin\"\n",
    "                                % (e+1, i, len(dataloader_dict['train']), total_loss, accuracy, \n",
    "                                optimizer.param_groups[0]['lr'], (time.time() - start_time_e) / 60))\n",
    "                        freq = 0\n",
    "                    freq += 1\n",
    "                if phase == 'valid':\n",
    "                    loss = F.cross_entropy(predicted_logit, trg)\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = predicted_logit.max(dim=1)\n",
    "                    val_acc += sum(predicted == trg).item() / predicted.size(0)\n",
    "        # Finishing iteration\n",
    "        if phase == 'valid':\n",
    "            val_loss /= len(dataloader_dict['valid'])\n",
    "            val_acc /= len(dataloader_dict['valid'])\n",
    "            print(\"[Epoch:%d] val_loss:%5.3f | Accuracy:%5.2f | spend_time:%5.2fmin\"\n",
    "                    % (e+1, val_loss, val_acc, (time.time() - start_time_e) / 60))\n",
    "            if not best_val_loss or val_loss < best_val_loss:\n",
    "                print(\"[!] saving model...\")\n",
    "                torch.save(model.state_dict(), \n",
    "                            os.path.join(save_path, f'model_saved.pt'))\n",
    "                best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.1 s, sys: 0 ns, total: 17.1 s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spm_ = spm.SentencePieceProcessor()\n",
    "spm_.Load(f\"{save_path}/m_text.model\")\n",
    "\n",
    "# Tokenizing\n",
    "test_encoded_list = [[bos_idx] + spm_.EncodeAsIds(text) + [eos_idx] for text in test_text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomtestDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.num_data = len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        src = self.data[index]\n",
    "        return src\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data\n",
    "    \n",
    "class test_PadCollate:\n",
    "    def __init__(self, pad_index=0, dim=0):\n",
    "        self.dim = dim\n",
    "        self.pad_index = pad_index\n",
    "\n",
    "    def pad_collate(self, batch):\n",
    "        def pad_tensor(vec, max_len, dim):\n",
    "            pad_size = list(vec.shape)\n",
    "            pad_size[dim] = max_len - vec.size(dim)\n",
    "            return torch.cat([vec, torch.LongTensor(*pad_size).fill_(self.pad_index)], dim=dim)\n",
    "\n",
    "        def pack_sentence(sentences):\n",
    "            sentences_len = max(map(lambda x: len(x), sentences))\n",
    "            sentences = [pad_tensor(torch.LongTensor(seq), sentences_len, self.dim) for seq in sentences]\n",
    "            sentences = torch.cat(sentences)\n",
    "            sentences = sentences.view(-1, sentences_len)\n",
    "            return sentences\n",
    "\n",
    "        src = batch\n",
    "        return pack_sentence(src)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self.pad_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_dict = {\n",
    "    'test' : CustomtestDataset(test_encoded_list)\n",
    "}\n",
    "\n",
    "dataloader_test_dict = {\n",
    "    'test' : DataLoader(dataset_test_dict['test'], collate_fn=test_PadCollate(), drop_last=False,\n",
    "                       batch_size = 1024, num_workers=4, shuffle=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop\n",
    "\n",
    "len(dataloader_test_dict['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "eval_list = []\n",
    "model.eval()\n",
    "for src in dataloader_test_dict['test']:\n",
    "    src = src.to(device)\n",
    "    predicted_logit = model(src)\n",
    "    _, prediction = predicted_logit.max(dim=1)\n",
    "    eval_list.append(prediction.tolist())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_list = sum(eval_list, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_submission['prediction'] = flatten_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
