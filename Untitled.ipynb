{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://ftp.daumkakao.com/pypi/simple\n",
      "Collecting tensorflow_addons\n",
      "\u001b[?25l  Downloading http://mirror.kakao.com/pypi/packages/b3/f8/d6fca180c123f2851035c4493690662ebdad0849a9059d56035434bff5c9/tensorflow_addons-0.11.2-cp36-cp36m-manylinux2010_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 1.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typeguard>=2.7\n",
      "  Downloading http://mirror.kakao.com/pypi/packages/f3/28/cc6df4c26d14c338c9744dc510a8c7f1a9115f8233e7602cca140a61430c/typeguard-2.10.0-py3-none-any.whl\n",
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.11.2 typeguard-2.10.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentencepiece\n",
    "# !pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "sample_submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_num(text):\n",
    "    return re.sub(r'[^a-zA-z0-9\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = train['question_text'].str.lower().apply(alpha_num)\n",
    "test_text_list = test['question_text'].str.lower().apply(alpha_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop 1 finish\n",
      "loop 2 finish\n",
      "loop 3 finish\n",
      "loop 4 finish\n"
     ]
    }
   ],
   "source": [
    "valid_percent = 0.2\n",
    "\n",
    "data_len = len(train)\n",
    "test_index = list(range(len(test)))\n",
    "\n",
    "valid_index = np.random.choice(range(data_len), int(data_len*valid_percent), replace=False)\n",
    "train_index = list(set(range(data_len)) - set(valid_index))\n",
    "\n",
    "train_text_list = [text_list[i] for i in train_index]\n",
    "print(\"loop 1 finish\")\n",
    "valid_text_list = [text_list[i] for i in valid_index]\n",
    "print(\"loop 2 finish\")\n",
    "test_text_list = [test_text_list[i] for i in test_index]\n",
    "print(\"loop 3 finish\")\n",
    "\n",
    "train_label_list = [train['target'].tolist()[i] for i in train_index]\n",
    "print(\"loop 4 finish\")\n",
    "valid_label_list = [train['target'].tolist()[i] for i in valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './save'\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_tensor = tf.convert_to_tensor(train_label_list)\n",
    "valid_label_tensor = tf.convert_to_tensor(valid_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=int32, numpy=\n",
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_tensor[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 45000\n",
    "pad_idx = 0\n",
    "bos_idx = 1\n",
    "eos_idx = 2\n",
    "unk_idx = 3\n",
    "\n",
    "if not os.path.isfile(f'{save_path}/m_text.vocab'):\n",
    "    # 1) Make Korean text to train vocab\n",
    "    with open(f'{save_path}/text.txt', 'w') as f:\n",
    "        for text in train_text_list:\n",
    "            f.write(f'{text}\\n')\n",
    "\n",
    "\n",
    "    # 2) SentencePiece model training\n",
    "    spm.SentencePieceProcessor()\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f'--input={save_path}/text.txt --model_prefix={save_path}/m_text'\n",
    "        f'--vocab_size={vocab_size} --character_coverage=0.9995 '\n",
    "        f'--model_type=bpe --split_by_whitespace=true '\n",
    "        f'--pad_id={pad_idx} --unk_id={unk_idx} '\n",
    "        f'--bos_id={bos_idx} --eos_id={eos_idx}'\n",
    "    )\n",
    "\n",
    "    vocab_list = list()\n",
    "    with open(f'{save_path}/m_text_{vocab_size}.vocab') as f:\n",
    "        for line in f:\n",
    "            vocab_list.append(line[:-1].split('\\t')[0])\n",
    "    word2id_spm = {w: i for i, w in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentencePiece model load\n",
    "spm_ = spm.SentencePieceProcessor()\n",
    "spm_.Load(f\"{save_path}/m_text.model\")\n",
    "\n",
    "# Tokenizing\n",
    "train_encoded_list = [[bos_idx] + spm_.EncodeAsIds(text) + [eos_idx] for text in train_text_list]\n",
    "valid_encoded_list = [[bos_idx] + spm_.EncodeAsIds(text) + [eos_idx] for text in valid_text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = tf.keras.preprocessing.sequence.pad_sequences(train_encoded_list, padding='post', maxlen=500)\n",
    "valid_tensor = tf.keras.preprocessing.sequence.pad_sequences(valid_encoded_list, padding='post', maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tensor = tf.convert_to_tensor(valid_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 500\n",
    "BATCH_SIZE = 512\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "steps_per_epoch = 500 // BATCH_SIZE\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_tensor, train_label_tensor)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((valid_tensor, valid_label_tensor))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([512, 500]), TensorShape([512]))"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_label_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_label_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Model_1(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units, batch_sz):\n",
    "        super(My_Model_1, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.units = units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units, \n",
    "                                      return_sequences=True,\n",
    "                                      return_state = True)\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "        self.last_one = tf.keras.layers.Dense(2, activation='softmax')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = self.initialize_hidden_state())\n",
    "        \n",
    "        query = tf.expand_dims(state, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query) + self.W2(output)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        \n",
    "        concat = tf.concat([context_vector, state], axis=-1)\n",
    "        \n",
    "        result = self.last_one(concat)\n",
    "        \n",
    "#         score = self.W1(state)\n",
    "#         repeat_score = tf.repeat(tf.expand_dims(score, axis=1), 1024, axis=1)\n",
    "#         print(output.shape, repeat_score.shape)\n",
    "#         score_1 = tf.tensordot(repeat_score, output, axes=[[1], [0]])\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sample = My_Model_1(vocab_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sample.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer='adam',\n",
    "                    metrics=[tf.keras.metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 2040 steps, validate for 510 steps\n",
      "Epoch 1/2\n",
      "2040/2040 [==============================] - 1975s 968ms/step - loss: 0.1261 - sparse_categorical_accuracy: 0.9509 - val_loss: 0.1132 - val_sparse_categorical_accuracy: 0.9551\n",
      "Epoch 2/2\n",
      "2040/2040 [==============================] - 1971s 966ms/step - loss: 0.1085 - sparse_categorical_accuracy: 0.9565 - val_loss: 0.1106 - val_sparse_categorical_accuracy: 0.9559\n"
     ]
    }
   ],
   "source": [
    "model_train = model_sample.fit(dataset, epochs=2, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm_ = spm.SentencePieceProcessor()\n",
    "spm_.Load(f\"{save_path}/m_text.model\")\n",
    "\n",
    "# Tokenizing\n",
    "test_encoded_list = [[bos_idx] + spm_.EncodeAsIds(text) + [eos_idx] for text in test_text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = tf.keras.preprocessing.sequence.pad_sequences(test_encoded_list, padding='post', maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = tf.convert_to_tensor(test_tensor)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_tensor)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.argmax(model_sample.predict(test_dataset), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my__model_1_70\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_70 (Embedding)     multiple                  11520000  \n",
      "_________________________________________________________________\n",
      "gru_70 (GRU)                 multiple                  3938304   \n",
      "_________________________________________________________________\n",
      "dense_128 (Dense)            multiple                  1049600   \n",
      "_________________________________________________________________\n",
      "dense_129 (Dense)            multiple                  1049600   \n",
      "_________________________________________________________________\n",
      "dense_130 (Dense)            multiple                  1025      \n",
      "_________________________________________________________________\n",
      "dense_131 (Dense)            multiple                  4098      \n",
      "=================================================================\n",
      "Total params: 17,562,627\n",
      "Trainable params: 17,562,627\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_sample.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
